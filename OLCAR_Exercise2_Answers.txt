
Exercise 2a: Mountain Car
===========================

---------------------------
Question 1: Build a probabilistic model of the system. What is the stochastic element in the modeling process and what is its significance? What modeling parameters have the most effect on the quality of the solution?

The probabilistic elemet comes from the fact that there are several modeling iterations (Parameters.modeling_iter). For each action and for each state, the value of the corresponding matrices P_s_sp_a and R_s_s_a are updated multiple times.



---------------------------
Question 2: Implement the Generalized Policy Improvement algorithm introduced in Section 2.8.3. Use appropriate terminal conditions for Policy Evaluation and Policy Improvement processes and implement the Policy Iteration and the Value Iteration algorithms. Think about what the optimal solution to this task should be for the Mountain Car system. Was the learning algorithm able to find this solution? If not, why
do you think that is the case?



---------------------------
Question 3: Now build a deterministic discrete state=action space model of the system (i.e. set the number of modeling iterations to 1). Is it possible to find a policy which reaches the goal? What problems are faced when discretizing the system in this way?



Exercise 2b: Cliff World
===========================

---------------------------
Question 4: First implement the Monte Carlo algorithm. Test the algorithm using different values of epsilon. What impact does epsilon have on the solution? Can the algorithm find the optimal greedy policy?

The value of epsilon characterizes how soft the policy is. With a non-zero probability (epsilon/|U|) a non-greedy policy - a policy which does not directly optimize the value function - is taken.
small epsilon:	exploitation
big epsilon:	exploration 


---------------------------
Question 5: Now implement the Q=Learning algorithm. First test the algorithm with decreasing exploration during learning (i.e. k_epsilon < 1). Next, test the algorithm using constant exploration during learning (i.e. k_epsilon = 1). How does k_epsilon influence the solution?

Configuring a constant exploration during learning (i.e. k_epsilon = 1) still allows to find the solution.
In contrast to the case where the k_epsilon decreases during learning, there are much more "oscillations" in the total reward function.
This can be explained with the always constant probability of choosing an non-greedy action, other than the currently optimal action.
In the case where the epsilon decreases, the longer the learning phase is, the smaller the probability gets, that the algorithm chooses a non-greedy action.
As already described before, this leads to a "smoother" evolution of the total reward function.


---------------------------
Question 6: Compare the computational efficiency and performance of the Monte Carlo and Q=Learning algorithms. What are the fundamental reasons why one algorithm performs better than the other?

The off-policy Q-Learning performs much better than the on-policy Monte-Carlo algorithm.

Parts in Monte-Carlo algorithm which is of big computational effort:
- Q-Learning combines "Policy Evaluation" and "Policy Iteration" into 1 single step, where for Monte-Carlo this is made one after each other


---------------------------